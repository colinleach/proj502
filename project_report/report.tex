%%
%\documentclass[twocolumn, tighten]{aastex63}
%\documentclass[twocolumn, twocolappendix, tighten]{aastex631}
%\documentclass[twocolumn, twocolappendix, tighten]{aastex631}
\documentclass[preprint]{aastex631}

%\newcommand{\vdag}{(v)^\dagger}
%\newcommand\aastex{AAS\TeX}
%\newcommand\latex{La\TeX}

\newcommand{\tsub}[1]{\textsubscript{#1}}
\newcommand{\tsup}[1]{\textsuperscript{#1}}
\newcommand{\so}{\qquad \implies \qquad}
\newcommand{\todo}{\color{red}{TODO}\color{black}\hspace{2mm}}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\shorttitle{Galaxy Zoo Morphology}
\shortauthors{Colin Leach}

\graphicspath{{figures/}{./}}

\begin{document}

\title{Term Project \\Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning}

\author[0000-0003-3608-1546]{Colin Leach}

\begin{abstract}

Astronomical survey data has expanded impressively since the era when professional astronomers could keep up with it by themselves. As an early enhancement, Galaxy Zoo used large numbers of amateur volunteers for classification of SDSS results, more recently extended to HST, CANDELS and DECaLS images. To scale further for the Rubin/Euclid era, that approach needs to be supplemented with ML techniques to use the volunteers more efficiently. \citet{walmsley_galaxy_2020} attempts to develop such a hybrid human/ML system. The current term project attempts to reproduce and (perhaps) extend this work.\\

\end{abstract} 

\section{Introduction} \label{sec:intro}

%\paragraph{Literature Background}

The Galaxy Zoo (GZ) project started as an attempt to scale manual classification of SDSS images by recruiting citizen scientists \citep{2008MNRAS.389.1179L, lintott_crowd_2019}. This succeeded beyond expectations, but is struggling to handle new data sources: DES, Rubin, Euclid, etc. Volunteer input is increasingly regarded as a finite and valuable resource, which needs to be used more efficiently \citep{2020IAUS..341...99D}.

Sorting galaxies by color has been done for decades (blue spirals, red ellipticals), though this has been criticized as inaccurate \citep{smethurst_quantifying_2022}. Other approaches include radial brightness curves, looking for central bulges and bars. Attempts to use neural networks to classify morphology go back at least to a \href{https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge}{Kaggle challenge} in 2014, won by \citet{2015MNRAS.450.1441D}. The concept of transfer learning, using older surveys to train models for a newer one, was explored by \citet{2019MNRAS.484...93D} and later by \citet{walmsley_galaxy_2020} (hereafter W+20), discussed in more detail in \citet{2021MNRAS} (hereafter W+21). These all focus on visual images (or their equivalents redshifted to IR), but \citet{2021arXiv211104353F} discusses an exchange of techniques with radio astronomy. A broader review of ML in astronomy is given in \citet{2020WDMKD..10.1349F}.

The GZ2 catalog \citep{willett_galaxy_2013, 2016MNRAS.461.3663H} is based on SDSS DR7. Later catalogs include Galaxy Zoo: Hubble \citep{2017MNRAS.464.4176W}, CANDELS \citep{2017MNRAS.464.4420S} and DECaLS (W+21).

\section{Aims}

In W+20, an attempt is described to develop a human-machine hybrid strategy for galaxy morphology:
\begin{itemize}
	\item Use the large Galaxy Zoo 2 (GZ2) catalog to train a CNN that can classify SDSS images.
	\item Use this model as a starting point to classify new data sources and formats, using only modest amounts of labeling from human volunteers to fine-tune the model.
\end{itemize}


\section{Code} \label{sec:code}

\subsection{Zoobot Code} \label{sec:zcode}

Python/Tensorflow code is on Github\footnote{https://github.com/mwalmsley/galaxy-zoo-bayesian-cnn} \citep{walmsley_mwalmsleygalaxy-zoo-bayesian-cnn_2019}, claiming to be an exact copy of that used for W+20.

Perhaps more interesting is the zoobot repo\footnote{https://github.com/mwalmsley/zoobot}, a fork which is still under active development. This extends the project to DECaLS (Dark Energy Camera Legacy Survey) data, as described in W+21. It also has \href{https://zoobot.readthedocs.io/}{much better documentation} than the earlier code.

\subsection{Code for Term Project}

Python code and documentation associated with ASTR 502 is available on Github\footnote{https://github.com/colinleach/proj502}. This aims to cover both GZ2, as in W+20, and DECaLS, as in W+21.


\section{Computation} \label{sec:comp}

W+20 reports that GZ2 training was carried out on a p2.xlarge EC2 instance with K80 GPU, taking about 8 hours. For DECaLS, the GPU was upgraded to a V100.

Experiments with the GPUs available to me at the start of this project rapidly proved that 2GB of GPU memory is wholly inadequate for training a CNN. Upgrading to a 6GB GTX 1660 (far from state of the art, but affordable and compatible with the existing motherboard and PSU) allowed some progress. This still proved limiting for batch size as discussed below, but was useful for debugging before moving to Colab. Making predictions from a pre-trained model was less demanding and worked well on local hardware.

For training runs, the free tier of Colab had few advantages over local hardware. Reaching the target batch size of 128 used a 16GB V100 GPU for long periods ($>12$h), with Colab upgraded to the Pro and later Pro+ tiers.

Only one attempt was made to use a Colab TPU, unsuccessfully. Getting this working will require copying large files to the TPU before each run because (unlike a GPU) this cannot be linked directly to Google Drive storage.

There are some files in the zoobot repo for setting up a docker image on an EC2 instance. This has not been tried as part of the term project but would be interesting in future.

\section{Term Project Strategy} \label{sec:goals}

My time is less valuable than for faculty or grad students, so goals are open-ended depending on energy, enthusiasm and (hopefully) competence. Roughly:
\begin{enumerate}
	\item Get the published Keras code running on my local machine, using whatever cut-down training sets (GZ2 and DECaLS) prove viable.
	\item Deploy the code on either AWS or Google.
	\item Repeat for PyTorch code
	\item Extend the model to other data such as Hubble or CANDELS, for which there is already some GZ classification.
	\item Rewrite using other languages and frameworks, for my education: Julia with Flux; maybe F\#/ML.NET.
\end{enumerate}

Not all of this will be complete before the end of the semester (an understatement).

\section{Algorithms} \label{algorithms}

\subsection{What are we trying to predict?}

Galaxy Zoo catalogs are not just a simple classification, such as elliptical vs spiral. The questions posed to volunteers have evolved over the years, though all follow a decision tree which depends on the answer to previous questions. The version for DECaLS DR5 is shown in Figure \ref{fig:decals_decisions}; GZ2 is similar but slightly simpler.

\begin{figure}[htb!]
	\epsscale{0.7}
	\plotone{decals_decisions}
	\caption{The GZ decision tree used for DECaLS DR5
		\label{fig:decals_decisions}}
\end{figure}

In the Python code this is represented by two dictionaries: for questions/answers and for dependencies. The Q\&A version for DECaLS is shown below: keys are questions, values are lists of allowed answers (as a suffix which will be appended to the question). The dependency dictionary lists previous questions that would allow the current question to be reached.


\begin{lstlisting}[language=Python]
decals_pairs = {
	'smooth-or-featured': ['_smooth', '_featured-or-disk', '_artifact'],
	'disk-edge-on': ['_yes', '_no'],
	'has-spiral-arms': ['_yes', '_no'],
	'bar': ['_strong', '_weak', '_no'],
	'bulge-size': ['_dominant', '_large', '_moderate', '_small', '_none'],
	'how-rounded': ['_round', '_in-between', '_cigar-shaped'],
	'edge-on-bulge': ['_boxy', '_none', '_rounded'],
	'spiral-winding': ['_tight', '_medium', '_loose'],
	'spiral-arm-count': ['_1', '_2', '_3', '_4', '_more-than-4', '_cant-tell'],
	'merging': ['_none', '_minor-disturbance', '_major-disturbance', '_merger']
}
\end{lstlisting}

Thus there are 10 possible questions (not all of which will be asked in each case), and 34 possible answers. Each answer has its own field in the input to the model (in addition to an identifier and the image), and the training output includes a weighting for each. The prediction step then takes a new galaxy and, in principle, produces a probability for each of the possible answers. This is discussed in more detail in section \ref{sec:predictions}.

\subsection{ML model} \label{model}

This evolved during the development of Zoobot. For W+20 and the mwalmsley/galaxy-zoo-bayesian-cnn repo, the architecture was a cut-down version of VGG16 \citep{2014arXiv1409.1556S}. For W+21 and mwalmsley/zoobot it had been updated to EfficientNet-B0 \citep{2019arXiv190511946T}. The latter was used in the current work.

\begin{deluxetable}{llc}[htb!]
	\tablecaption{Output from TensorFlow model.summary()
		\label{tbl:tfmodel}}
	\tablewidth{0pt}
	\tablehead{
		\colhead{Layer} & \colhead{Output Shape} & \colhead{Param \#} 
	}
	\startdata
	random rotation & (None, 300, 300, 1) & 0 \\        
	random flip & (None, 300, 300, 1) & 0 \\       
	random crop & (None, 224, 224, 1) & 0 \\        
	sequential 1 & (None, 7, 7, 1280) & 4048988 \\
	global avg pooling 2d & (None, 1280) & 0 \\      
	top dropout & (None, 1280) & 0 \\        
	dense & (None, 34) & 43554     
	\enddata
\end{deluxetable} \vspace{-10mm}

The model summary reported by Keras for DECaLS training is shown in Table \ref{tbl:tfmodel}. The first three layers are fairly standard image preprocessing steps (data augmentation). The EfficientNet component is all in the ``sequential 1'' layer, shown in more detail in Figure \ref{fig:efficientnet}, followed by pooling and dropout. The final dense layer gives a 34-component output, corresponding to the possible answers from the volunteers.

\begin{figure}
	\epsscale{0.7}
	\plotone{efficientnet-b0}
	\caption{EfficientNet-B0, used as published \label{fig:efficientnet}}
\end{figure}



\subsection{Loss Function}

In W+20 volunteer responses were modeled as binomially distributed

\begin{equation}
 \mathcal{L} = \int \mathrm{Bin} (k | \rho, N)\, \mathrm{Beta} (\rho | \alpha, \beta)\, d\alpha\, d\beta 
\end{equation}

To address some limitations, W+21 modified this to use the multinomial equivalent of each function, replacing $\mathrm{Binomial} (k | \rho, N)$ with $\mathrm{Multinomial} (\vec{k} | \vec{\rho}, N)$ and $\mathrm{Beta} (\rho | \alpha, \beta)$ with $\mathrm{Dirichlet} (\vec{\rho} | \vec{\alpha})$:

\begin{equation}
 \mathcal{L} = \int \mathrm{Multi} (\vec{k} | \vec{\rho}, N)\, \mathrm{Dirichlet} (\vec{\rho} | \vec{\alpha})\, d\vec{\alpha} 
\end{equation}

The parameters are now vectors with one element per answer.

\subsection{Output}

Results are saved in a Tensorflow binary format, suitable for use as a starting point for making predictions. They are also used by the TensorBoard utility to make training plots as in Figure \ref{fig:train_plots}.


\section{Workflow}

In outline, these are the steps required:

\begin{enumerate}
	\item Get the Galaxy Zoo catalog data for each survey of interest.
	\item Get the image files (JPG or PNG), one per galaxy in the classification.
	\item Make a combined catalog, including a path to the image on disk plus the data fields relevant to the model.
	\item Split the galaxies into train, evaluate and test sets. For each, prepare a binary-format tensor (tfrecord) file containing image and classification data.
	\item Train the model on the train and evaluate sets.
	\item Predict results with the test set and compare with the GZ classification.
\end{enumerate}

The following subsections address each of these in more detail.

\subsection{GZ data}

GZ2 catalog files were downloaded from the Galaxy Zoo website.
There are a total of 243,500 rows in the table. For better consistency, only those marked 'original' in the sample field were used in subsequent analyses, a set of 211,922.

Extensive DECaLS data is available from Zenodo \citep{walmsley_mike_2020_4573248}. For this study the file 'gz\_decals\_volunteers\_5.parquet' was used, a total of 253,286 rows.

For maximum flexibility (and because old habits die hard), all this data was stored in a PostgreSQL database, running locally.

\subsection{Images} \label{images}

The GZ team do not make their images library publicly available, so the RA and Dec fields in the GZ2 dataset were used to fetch $424 \times 424$ JPG cutouts from the \href{http://skyserver.sdss.org/dr14/SkyServerWS/ImgCutout/getjpeg}{SDSS SkyServer}. Because Zoobot is currently configured to use PNG images, the Python code converted each file with PIL. The PNG files totaled around 33 GB, much more than the corresponding JPG files.

DECaLS DR5 images were downloaded from Zenodo \citep{walmsley_mike_2020_4573248} as 4 ZIP files, unpacked to 272,725 $424 \times 424$ PNG files totaling 83 GB. 

File paths and some metadata was stored in PostgeSQL.

Although the survey telescopes are at different latitudes (SDSS at Apache Point, NM; DECaLS at Cerro Tololo, Chile) there is significant overlap in coverage (Figure \ref{fig:coverage}).

\begin{figure}[htb!]
	%\epsscale{0.9}
	\plotone{coverage.png}
	\caption{Sky locations of images used for each survey
		\label{fig:coverage}}
\end{figure}



\subsection{Combined catalog}

Having everything in PostgreSQL makes it easy to join the data and image tables and select the desired columns. Each resulting dataset was converted to a pandas DataFrame and saved as a CSV file. This is quick and produces relatively small files (around 35 MB).

Zoobot requires the columns to have the correct names and appear in the correct order. A galaxy identifier is in 'id\_str' and a full path to the PNG is in 'file\_loc', then the remaining columns contain total votes cast for each answer in the GZ decision tree.

\subsection{Tensor shards} \label{shards}

Before training, input data needs to be converted to a tfrecord format that TensorFlow can read quickly. The combined catalog is split into train, evaluate and test sets; for this project a 7:2:1 ratio was used. For each set, image files are read and undergo initial cropping and resizing before combining with the GZ votes and written to binary tfrecord files. This took around 1 hour per survey (i9 processor, local SSD storage) and the output files total about 100 GB.

For debugging, a much smaller GZ2 shard set was also created, with fewer records and low-resolution images. This proved valuable in quickly finding some minor bugs in the current Zoobot repo: apparently it was tested mainly with DECaLS data and there are some typos and omissions in the GZ2 code. Accordingly, from this point the project uses my fork of the mwalmsley/zoobot repo. A PR with the corrections will be submitted upstream once everything is working correctly.

\subsection{Training Locally}

\begin{figure}[h!]
	\gridline{\fig{gz2_train_64.pdf}{0.3\textwidth}{(a)}
		\fig{decals_train.pdf}{0.3\textwidth}{(b)}}
	\gridline{\fig{gz2_train_32.pdf}{0.3\textwidth}{(c)}
		\fig{decals_train_2.pdf}{0.3\textwidth}{(d)}}
	\caption{Training runs for GZ2 (left) and DECaLS (right) \label{fig:train_plots}}
\end{figure}

As expected, this proved a slow step in the workflow and exposed the limitations of the local (6 GB) GPU. A batch size of 128 was used in the published work. For GZ2, this caused an immediate GPU out-of-memory error. Dropping to batches of 64 was more successful, as in Figure \ref{fig:train_plots}(a), though this used most of the available GPU memory. Progressing at about 10 min/epoch, the training loss drops smoothly and reached stopping criteria (10 epochs without improvement) after 54 epochs, 8.3 hours. However, the validation loss is noisy and suggests poor generalization.



For DECaLS, the batch size needed to be reduced to 32 to fit in GPU memory. Training is slower (about 30 min/epoch) but the results are more encouraging, as shown in Figure \ref{fig:train_plots}(b). After some initial spikes, the validation loss tracks closely with the training loss. This run failed to reach stopping criteria within the epoch limit (40 epochs, nearly 19 hours). A longer second run, shown in Figure \ref{fig:train_plots}(d), terminated successfully. Differences between the two runs mainly illustrate the stochastic variation in this method.

\begin{figure}[h!]
	\gridline{\fig{J110001.05+010644.0_3.pdf}{0.9\textwidth}{(a)}}
%	\gridline{\fig{J110459.08+121820.3.pdf}{0.6\textwidth}{(b)}}
	\gridline{\fig{J110254.80+325229.0_3.pdf}{0.9\textwidth}{(b)}}
	\caption{Raw images used for GZ2 at default resolution (left) and zoomed (center); DECaLS (right) \label{fig:images}}
\end{figure}

It was not immediately clear why the DECaLS run looks better than the GZ2 run. Preparation of data shards used the same code and no error has yet been found. Other hypotheses include the different batch size and different image size and quality. Batch size was easiest to test, so GZ2 training was repeated with batches of 32 as in Figure \ref{fig:train_plots}(c). This was not encouraging: training now looks worse without validation looking better.

Images obtained from DECaLS are inherently higher resolution and deeper than those from SDSS used in GZ2 (bigger telescope, newer camera). Indeed, ambiguous galaxies are consistently reported by volunteers as more featured in GZ DECaLS than GZ2 (W+21, Figure 5). Training was also carried out on differently sized images: $300 \times 300$ for DECaLS and $256 \times256$ for GZ2.

The respective catalogs share no common ID field, but a match on RA/Dec coordinates identified 132,722 images which are in both data sets. A few representative examples are shown in Figure \ref{fig:images}. Clearly there is a major difference in quality.

By default, the SDSS cutout service supplies images at a scale of 0.4 arcsec/pixel. As a first attempt to do better, new images were downloaded at 0.1 arcsec/pixel, with the SDSS server handling any necessary pixel interpolation.

There are also significant differences in image preparation. W+20 gives little detail about this for GZ2, so the simple method described in section \ref{images} was followed.  In contrast, W+21 describes a more complex process, starting from FITS data files at native telescope resolution. Something equivalent may be possible for SDSS, but  this is not an urgent priority for an ASTR 502 term paper. From this point only the DECaLS survey is used.


\subsection{Training on Colab} \label{sec:colabtrain}

All relevant files were copied to Google Drive (more than 100 GB), from where they could be mounted in a Colab notebook. Training with batch size 128 used around 95\% of the 16GB GPU memory. At around 15 min/epoch, both GZ2 and DECaLS took approximately 12 h to converge. Results, shown in Figure \ref{fig:train_colab}, are broadly similar to those obtained locally.

\begin{figure}
	\epsscale{0.7}
	\plottwo{gz2_train_128.pdf}{decals_train_bs128.pdf}
	\caption{Colab training runs for GZ2 (left) and DECaLS (right) \label{fig:train_colab}}
\end{figure}


\section{Predictions} \label{sec:predictions}

When preparing shards as in Section \ref{shards}, the input catalog was divided into train, validate and test sets, then each was encoded as binary tfrecord shards including the graphical image. This helped speed training but is largely pointless for predictions on the test set. Instead, this step used a CSV (or alternatively feather) file as input which included full paths to the relevant images.

For DECaLS, the test set contains around 43k galaxies. Using the model pretrained on Colab in the previous step, predictions were generated for all of these in around 11 min. Output was to an HDF5 file; CSV is also possible but less convenient.

The raw output proved surprising. For each galaxy, in addition to a full path to the image the software documentation implied there would be an array of 34 probabilities, corresponding the set of possible Q\&A pairs in the GZ survey. Instead, there was a $34 \times 5$ array. From W+21, it appears that the final random dropout step of the model (Figure \ref{model}) runs multiple times and an output is stored for each of them, giving an assessment of reproducibility. In practice, the five values generally had a small standard deviation, so all further analyses used the mean.

It is also somewhat unclear what the numbers represent. Probabilities were expected, with the options for each question adding up to 1 (or  100\%). Instead the sum is highly variable and the distribution centers around roughly 70. Taking a view that the relative values are most important, everything was simply normalized to sum to 1 for clarity. 

\subsection{Reviewing predictions: single galaxy}

To get a first feel for the results, a galaxy was chosen at random (only ensuring that it was one with a reasonably clear image). The prediction output is shown in Table \ref{tbl:single_pred}. Answers with at least an 80\% confidence are bolded. In all cases these agree with the consensus of the GZ volunteers, as does ``spiral-winding''.

\begin{deluxetable}{ll}[htb!]
	\tablecaption{Predictions for a single random galaxy, in descending order of probability
		\label{tbl:single_pred}}
	\tablewidth{0pt}
	\tablehead{
		\colhead{Question} & \colhead{Predicted answers (probability)}
	}
	\startdata
	\textit{smooth-or-featured} & \textbf{featured-or-disk} (0.89), smooth (0.08), artifact (0.03) \\
	\textit{disk-edge-on} & \textbf{no} (0.93), yes (0.07) \\
	\textit{has-spiral-arms} & \textbf{yes} (0.95), no (0.05) \\
	\textit{bar} & weak (0.44), no (0.41), strong (0.16) \\
	\textit{bulge-size} & moderate (0.62), small (0.27), large (0.08), none (0.02), dominant (0.01) \\
	\textit{how-rounded} & in-between (0.57), cigar-shaped (0.39), round (0.05) \\
	\textit{edge-on-bulge} & \textbf{rounded} (0.82), none (0.14), boxy (0.04) \\
	\textit{spiral-winding} & medium (0.57), loose (0.23), tight (0.20) \\
	\textit{spiral-arm-count} & \textbf{2} (0.91), cant-tell (0.04), 1 (0.02), 3 (0.01), 4 (0.01), more-than-4 (0.01) \\
	\textit{merging} & \textbf{none} (0.83), minor-disturbance (0.12), major-disturbance (0.03), merger (0.02)
	\enddata
\end{deluxetable} \vspace{-10mm}

Of the other questions, the volunteers by small majorities favored no bar and small bulge, the second choices of the predictor. The remaining two show the limitations of this crude analysis: ``how-rounded'' is not relevant to disks and ``edge-on-bulge'' is only relevant if the disk is edge on. Most volunteers would therefore have bypassed these questions, as shown in the decision tree of Figure \ref{fig:decals_decisions}.   


\subsection{Reviewing predictions: all galaxies}

Given the complexities of inter-dependent questions, a simple starting point would be to focus on question 1, ``smooth-or-featured'', which all volunteers have to answer. An astronomer might prefer ``elliptical or disk'', but the chosen wording works for a wider public\footnote{Award-winning postdoc and podcaster Dr Becky Smethurst, a co-author on several of these papers, likes the name ``boring blobby things'' for ellipticals. It remains to been seen whether this will help her get a faculty position.}. Table \ref{tbl:smoothdisk} shows the confusion matrix, with agreement between volunteers and the model for 88\% of the images.

\begin{deluxetable}{lccc}[htb!]
	\tablecaption{Confusion matrix for ``smooth or featured''.
		\label{tbl:smoothdisk}}
	\tablewidth{0pt}
	\tablehead{
		 & \colhead{smooth} & \colhead{featured-or-disk} & \colhead{artifact}
	}
	\startdata
		smooth & \textbf{27411} & 2183 & 159\\
		featured-or-disk & 2365 & \textbf{10356} & 95\\
		artifact & 329 & 54 & \textbf{321}
	\enddata
\end{deluxetable} \vspace{-5mm}

Similarly, all volunteers are asked about possible mergers, with the results in Table \ref{tbl:merger}. In this case there is 87\% agreement. Off-diagonal values are asymmetric, with the model tending to favor some disturbance when volunteers prefer none. Note that this only uses the top-ranking choice and ignores confidence levels.

\begin{deluxetable}{lcccc}[htb!]
	\tablecaption{Confusion matrix for ``merging''.
		\label{tbl:merger}}
	\tablewidth{0pt}
	\tablehead{
		& \colhead{none} & \colhead{minor-disturbance} & \colhead{major-disturbance} & \colhead{merger}
	}
	\startdata
	none & \textbf{36093} & 133 & 230 & 785\\
	minor-disturbance & 2145 & \textbf{159} & 123 & 159\\
	major-disturbance & 441 & 31 & \textbf{287} &  155 \\
	merger & 1327 & 14 & 39 & \textbf{1152}
	\enddata
\end{deluxetable} \vspace{-10mm}

\newpage
\subsection{Using predictions: find top-scoring galaxies}

Where the prediction is uncertain between two choices, such as with 0.48 versus 0.45 probabilities, this may be a failure of the model. Perhaps more likely, it reflects genuine ambiguity in the image and a need for humans to take a closer look. Triaging images to use humans more efficiently was, of course, an original aim of this research. Also, there is some data to show that volunteers are often divided about the same ambiguous images, discussed in W+21. The GZ team classifies ``confident'' galaxies as those where at least 80\% of the volunteers agree on a classification \citep{2019MNRAS.484...93D}, and these are just a subset of the total.

An alternative approach may better use the strengths of the model: identify the top-$N$ galaxies confidently predicted to have a particular feature. This proved quick and easy, with a few examples shown in Figure \ref{fig:top5}. We could argue with the precise nomenclature, but this could be a simple and useful addition to survey data pipelines such as Rubin.\footnote{Other problems are harder. Several years ago I heard an IT engineer from LSST describe their hardware strategy as ``delay purchase orders until the last possible moment and hope that Moore's Law saves us''. We were both drinking beer at the time (but you already guessed that).}

\begin{figure}
	\gridline{\fig{merging_merger_5.pdf}{\textwidth}{Mergers}}
	\gridline{\fig{bar_strong_5.pdf}{\textwidth}{Strong bars}}
	\gridline{\fig{spiral-winding_loose_5.pdf}{\textwidth}{Loose spiral winding (mixed with tidal tails?)}}
	\caption{Top-5 prediction results for various parameters \label{fig:top5}}
\end{figure}


\section{Fine-tuning and Transfer Learning}

So far, the model is useful for most but not all of the parameters in the GZ catalogs. The next steps use this as a starting point to train other models, either to answer different questions with DECaLS, or to use other surveys as input.

\subsection{Fine-tuning for DECaLS}

Although the GZ DECaLS survey contains fields for rare features such as rings, lenses and dust lanes, theses are not included in the existing CNN model. Retraining the model to predict one of these features is a simple example of fine-tuning.

Zoobot includes an example set of 470 images and a CSV catalog matching file locations to a ring/no-ring field, balanced to have roughly equal numbers of each. Generating such a catalog needs some effort from humans, but far less than the big GZ classifications. 

These images were split into training and validation sets, the pretrained model from Section \ref{sec:colabtrain} was loaded with most of the layers frozen, then new final layers were added and the model retrained as a ring predictor. Locally, this ran with batch size 64 in less than 2 minutes, converging after 46 epochs.

Because the output is much simpler in this case, a single sigmoid value for each galaxy, the ring model is easier to understand than the full model. The loss function is a standard binary crossentropy and the accuracy on the class-balanced validation set converges to about 0.77.

The fine-tuned model was used to predict rings for the 272,725 galaxies in the DECaLS catalog: this time unbalanced, so the prior for having a ring is much less than $0.5$. Running locally, this took about 15 min.

Unfortunately, the results were disappointing. Prediction confidence was $<0.75$ in all cases, and on viewing the top 10 images there were no convincing rings.

Better results were reported in \citet{walmsley_galaxy_2022}, suggesting improvement is possible. Zoobot documentation gives a clue:

\begin{quote}
	 Zoobot includes two complete working examples:

	\textit{finetune\_minimal.py} shows how to partially finetune a model to classify ring galaxies using an example dataset. It is designed to be easy to understand.
	
	\textit{finetune\_advanced.py} solves the same problem with some additional tips and tricks: filtering the dataset, balancing the classes (rings are rare), and unfreezing the model.
\end{quote}

The training described earlier in this section followed the minimal example. Repeating using the advanced method is a current aim.

\subsection{Transfer Learning for other surveys}

Both GZ2 (SDSS) and GZ DECaLS models were trained from scratch. Repeating this for future large surveys such as Rubin is affordable in terms of computing resource, but would require too many volunteers to generate the training set. A better approach is to start from the existing DECaLS model with early layers frozen, then use a few hundred volunteer classifications on the new survey to refine the weights.

This has not yet been done for the ASTR 502 course. Exploring either Hubble or \href{http://candels.ucolick.org/}{CANDELS} data is planned for the near future, as GZ catalogs are available for both.


\section{PyTorch Code}

Any plans to write a PyTorch version of the original Keras code were abandoned when this was unexpectedly added to the zoobot repo by other developers. Updates appear to have slowed in recent weeks, so this is probably a good time start testing it (after 502 ends).  

\section{Conclusions}

The objectives of this term paper evolved significantly during the semester, not least because the lead author of W+20 continued to develop the code in ways that overlapped with my original plans.

Reproducing the published results was partly successful with SDSS data (as in W+20) and mostly successful with DECaLS data (W+21). Models were trained, and were used to make some apparently sensible predictions on tens of thousands on new galaxy images. This work remains incomplete as the semester ends, but some gaps are likely to be filled in the coming weeks.

Most importantly, published and peer-reviewed galaxy models are now available and have been shown to be usable in morphology classification. One can only hope that fewer astronomers in future will use ImageNet as a starting point for building new CNNs. 


\bibliography{GZML}{}
\bibliographystyle{aasjournal}


\end{document}

