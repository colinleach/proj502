%%
%\documentclass[twocolumn, tighten]{aastex63}
\documentclass[twocolumn, twocolappendix, tighten]{aastex631}

\newcommand{\vdag}{(v)^\dagger}
\newcommand\aastex{AAS\TeX}
\newcommand\latex{La\TeX}

\newcommand{\tsub}[1]{\textsubscript{#1}}
\newcommand{\tsup}[1]{\textsuperscript{#1}}
\newcommand{\so}{\qquad \implies \qquad}
\newcommand{\todo}{\color{red}{TODO}\color{black}\hspace{2mm}}


\shorttitle{title}
\shortauthors{Colin Leach}

\graphicspath{{./}{figures/}}

\begin{document}

\title{Term Project \\Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning}

\author[0000-0003-3608-1546]{Colin Leach}

\begin{abstract}

Astronomical survey data has expanded impressively since the era when professional astronomers could keep up with it by themselves. As an early enhancement, Galaxy Zoo used large numbers of amateur volunteers for classification of SDSS results, more recently extended to HST, CANDELS and DECaLS images. To scale further for the Rubin/Euclid era, that approach needs to be supplemented with ML techniques to use the volunteers more efficiently. \citet{walmsley_galaxy_2020} attempts to develop such a hybrid human/ML system. The current term project attempts to reproduce and (perhaps) extend this work.\\

\end{abstract} 

\section{Introduction} \label{sec:intro}

%\paragraph{Literature Background}

The Galaxy Zoo started as an attempt to scale manual classification of SDSS images by recruiting citizen scientists \citep{2008MNRAS.389.1179L}. This succeeded beyond expectations, but is struggling to keep up with new data sources: DES, Rubin, Euclid, etc. Volunteer input is increasingly regarded as a finite and valuable resource, which needs to be used more efficiently \citep{2020IAUS..341...99D}.

Sorting galaxies by color has been done for decades (blue spirals, red ellipticals), though this has been criticized as inaccurate \citep{smethurst_quantifying_2022}. Other approaches include radial brightness curves, looking for central bulges and bars. Attempts to use neural networks to classify morphology go back at least to a Kaggle challenge\footnote{https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge} in 2014, won by \citet{2015MNRAS.450.1441D}. The concept of transfer learning, using older surveys to train models for a newer one, was explored by \citet{2019MNRAS.484...93D} and later by W+20, discussed in more detail in \citet{2021arXiv211012735W}. These all focus on visual images (or their redshifted equivalents), but \citet{2021arXiv211104353F} discusses an exchange of techniques with radio astronomy. A broader review of ML in astronomy is given in \citet{2020WDMKD..10.1349F}.

GZ2 \citep{willett_galaxy_2013, 2016MNRAS.461.3663H} is based on SDSS DR7. Later catalogs include Galaxy Zoo: Hubble \citep{2017MNRAS.464.4176W}, CANDELS \citep{2017MNRAS.464.4420S} and DECaLS \citep{walmsley_galaxy_2022}.

\section{Aims}

In \citet{walmsley_galaxy_2020} (hereafter W+20), an attempt is described to develop a human-machine hybrid strategy for galaxy morphology:
\begin{itemize}
	\item Use the large Galaxy Zoo 2 (GZ2) catalog to train a CNN that can classify SDSS images.
	\item Use this model as a starting point to classify new data sources and formats, using only modest amounts of labeling from human volunteers to fine-tune the model.
\end{itemize}


\section{Code} \label{sec:code}

\subsection{Zoobot Code} \label{sec:zcode}

\textbf{Code:} All the Python/Tensorflow code is on Github\footnote{https://github.com/mwalmsley/galaxy-zoo-bayesian-cnn} \citep{walmsley_mwalmsleygalaxy-zoo-bayesian-cnn_2019}, claiming to be an exact copy of that used for W+20.

Perhaps more interesting is the zoobot repo\footnote{https://github.com/mwalmsley/zoobot}, a fork which is still under active development. This extends the project to DECaLS (Dark Energy Camera Legacy Survey) data, as described in \citet{2021MNRAS} (hereafter W+21). It also has much better documentation\footnote{https://zoobot.readthedocs.io/} than the earlier code.

\subsection{Code for Term Project}

Python code and documentation associated with ASTR 502 is available on Github\footnote{https://github.com/colinleach/proj502}. This aims to cover both GZ2, as in W+20, and DECaLS, as in W+21.


\section{Data} \label{sec:data}

\subsection{Catalog Data} 

GZ2 catalogs are available online\footnote{https://data.galaxyzoo.org/} in multiple formats, with 231 columns and nearly 300k rows as described in \citet{willett_galaxy_2013}.

The table used in this work was based on \citet{2016MNRAS.461.3663H}, downloaded as \href{https://zooniverse-data.s3.amazonaws.com/galaxy-zoo-2/zoo2MainSpecz.csv.gz}{a gzipped csv file}.  This is Table 5 in \citet{willett_galaxy_2013} and the column format is described in \href{https://data.galaxyzoo.org/data/gz2/zoo2MainSpecz.txt} an accompanying file

\subsection{Image Data:} 
The GZ team do not make their images library publicly available. However, each $512 \times 512$ image is available from the SDSS cutout service, using the ra/dec coordinates in the catalog table.

Before analysis, the images need to be downsampled to $256 \times 256$ monochrome pixels and stored as uint8.



\section{Computation} \label{sec:comp}

W+20 reports that GZ2 training was carried out on a p2.xlarge EC2 instance with K80 GPU, taking about 8 hours. For DECaLS, the GPU was upgraded to a V100.

Experiments with the GPUs available at the start of this project rapidly proved that 2GB of GPU memory is wholly inadequate for training a CNN. Upgrading to a 6GB GTX 1660 (\$450, and compatible with the existing motherboard and PSU) allowed some progress. However, this still proved limiting for batch size as discussed below.

\todo Colab and AWS


\section{Goals} \label{sec:goals}

My time is less valuable than for faculty or grad students, so goals are open-ended depending on energy, enthusiasm and (hopefully) competence. Roughly:
\begin{enumerate}
	\item Get the published code running on my local machine, using whatever cut-down training set proves viable.
	\item Deploy the code on either AWS or Google.
	\item Extend the model to other data such as Hubble, CANDELS, DECaLS, for which there is already some GZ classification.
	\item Think about newer CNN algorithms. The W+20 paper was submitted in 2019, but software decisions were made well before then and the authors admit it is not the latest technology.
	\item Rewrite using other frameworks, for my education. Most obviously PyTorch, but (unlike most astronomers!) I would also be interested to try Julia with Flux. As a stretch goal, I may try getting it working in F\#/ML.NET, but don't hold your breath waiting for that.
\end{enumerate}

I think we can assume that not all of this will be done before the end of the semester (an understatement).

\section{Workflow}

In outline, these are the steps required:

\begin{enumerate}
	\item Get the Galaxy Zoo catalog data for each survey of interest.
	\item Get the image files (JPG or PNG), one per galaxy in the classification.
	\item Make a combined catalog, including a path to the image on disk plus the data fields relevant to the model.
	\item Split the galaxies into train, evaluate and test sets. For each, prepare a binary tensor (tfrecord) file containing image and classification data.
	\item Train the model on the train and evaluate sets.
	\item Predict results with the test set and compare with the GZ classification.
\end{enumerate}

The following subsections address each of these in more detail.

\subsection{GZ data}

GZ2 files were downloaded from the Galaxy Zoo website.

\subsection{Images}

xxx

\bibliography{GZML}{}
\bibliographystyle{aasjournal}


\end{document}

